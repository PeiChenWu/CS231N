{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14452,
     "status": "ok",
     "timestamp": 1590102394947,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "z-3W8Wpgvake",
    "outputId": "cbf060f0-11c2-4224-be7d-ee34c6570322"
   },
   "outputs": [],
   "source": [
    "# Pytorch Example:\n",
    "!pip install cython\n",
    "# Install pycocotools, the version by default in Colab\n",
    "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqLd8hNAvakh"
   },
   "outputs": [],
   "source": [
    "#from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "#import matplotlib.image as mpimg\n",
    "#from matplotlib import pyplot as plt\n",
    "import json\n",
    "#import os\n",
    "#import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "#from PIL import Image\n",
    "import random\n",
    "#import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Xc17Ci3qNQo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJDiuLCnvalP"
   },
   "outputs": [],
   "source": [
    "#%%shell\n",
    "\n",
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "#git clone https://github.com/pytorch/vision.git\n",
    "#cd vision\n",
    "#&& git checkout v0.3.0\n",
    "\n",
    "#cp references/detection/utils.py ../\n",
    "#cp references/detection/transforms.py ../\n",
    "#cp references/detection/coco_eval.py ../\n",
    "#cp references/detection/engine.py ../\n",
    "#cp references/detection/coco_utils.py ../\n",
    "#print(sys.path)\n",
    "sys.path.append(\"drive/My Drive/cs231n_personal_project/vision/references/detection\")\n",
    "from engine import train_one_epoch,evaluate\n",
    "#from vision.references.detection.engine import train_one_epoch,evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "sys.path.insert(0, \"drive/My Drive/cs231n_personal_project/vision/torchvision\")\n",
    "\n",
    "#import torchvision\n",
    "#from torchvision.\n",
    "from models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import models\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1590102485122,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "N5s_qq_kvakj",
    "outputId": "94237b4f-a866-43fc-8b92-97d8378f17c4"
   },
   "outputs": [],
   "source": [
    "dataDir = \"drive/My Drive/cs231n_personal_project\"\n",
    "print(os.listdir(dataDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pN8kXpsZvakt",
    "outputId": "94048993-cac2-48e6-bc94-945dfe1c13cf"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dataDir, 'label_descriptions.json'), 'r') as file:\n",
    "    label_description = json.load(file)\n",
    "#label_description\n",
    "n_classes = len(label_description['categories'])\n",
    "n_attributes = len(label_description['attributes'])\n",
    "print(F\"Classes: {n_classes}, Attributes: {n_attributes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_6zEUsavakw"
   },
   "outputs": [],
   "source": [
    "categories_df = pd.DataFrame(label_description['categories'])\n",
    "attributes_df = pd.DataFrame(label_description['attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1788,
     "status": "ok",
     "timestamp": 1590102504023,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "VusgLpnSvalA",
    "outputId": "413185e7-0242-4e66-fe9f-c0cabbecc2b6"
   },
   "outputs": [],
   "source": [
    "#fashion_json = {}\n",
    "#fashion_json[\"train\"] = fashion_dict\n",
    "#import json, os\n",
    "#with open(os.path.join(\"output_16May2020\", 'dataset_train.json'), 'w') as fp:\n",
    "#    json.dump(fashion_json, fp)\n",
    "fashion_dict = []\n",
    "with open(os.path.join(dataDir, 'mini_dataset_train.json'), 'r') as file:\n",
    "    fashion_json = json.load(file)\n",
    "fashion_dict = fashion_json[\"train\"]\n",
    "#print(len(train_data_copy['ImageId'].unique().tolist()))\n",
    "print(fashion_dict[6]['attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2m-fpnbLvalC"
   },
   "outputs": [],
   "source": [
    "class FashionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, image_dir, transforms=None):\n",
    "        self.imgs = dataset\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        #self.height = height\n",
    "        #self.width = width\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        #self.imgs = list(sorted(os.listdir(os.path.join(root, \"train\"))))\n",
    "        #self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def rle_decode_string(self, string, h, w):\n",
    "        mask = np.full(h*w, 0, dtype=np.uint8)\n",
    "        annotation = [int(x) for x in string.split(' ')]\n",
    "        for i, start_pixel in enumerate(annotation[::2]):\n",
    "            mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "        mask = mask.reshape((h, w), order='F')\n",
    "        return mask\n",
    "\n",
    "    def rle2bbox(self, rle, shape):\n",
    "        '''\n",
    "        Get a bbox from a mask which is required for Detectron 2 dataset\n",
    "        rle: run-length encoded image mask, as string\n",
    "        shape: (height, width) of image on which RLE was produced\n",
    "        Returns (x0, y0, x1, y1) tuple describing the bounding box of the rle mask\n",
    "        \n",
    "        Note on image vs np.array dimensions:\n",
    "        \n",
    "            np.array implies the `[y, x]` indexing order in terms of image dimensions,\n",
    "            so the variable on `shape[0]` is `y`, and the variable on the `shape[1]` is `x`,\n",
    "            hence the result would be correct (x0,y0,x1,y1) in terms of image dimensions\n",
    "            for RLE-encoded indices of np.array (which are produced by widely used kernels\n",
    "            and are used in most kaggle competitions datasets)\n",
    "        '''\n",
    "        \n",
    "        a = np.fromiter(rle.split(), dtype=np.uint)\n",
    "        a = a.reshape((-1, 2))  # an array of (start, length) pairs\n",
    "        a[:,0] -= 1  # `start` is 1-indexed\n",
    "        \n",
    "        y0 = a[:,0] % shape[0]\n",
    "        y1 = y0 + a[:,1]\n",
    "        if np.any(y1 > shape[0]):\n",
    "            # got `y` overrun, meaning that there are a pixels in mask on 0 and shape[0] position\n",
    "            y0 = 0\n",
    "            y1 = shape[0]\n",
    "        else:\n",
    "            y0 = np.min(y0)\n",
    "            y1 = np.max(y1)\n",
    "        \n",
    "        x0 = a[:,0] // shape[0]\n",
    "        x1 = (a[:,0] + a[:,1]) // shape[0]\n",
    "        x0 = np.min(x0)\n",
    "        x1 = np.max(x1)\n",
    "        \n",
    "        if x1 > shape[1]:\n",
    "            # just went out of the image dimensions\n",
    "            raise ValueError(\"invalid RLE or image dimensions: x1=%d > shape[1]=%d\" % (\n",
    "                x1, shape[1]\n",
    "            ))\n",
    "\n",
    "        return x0, y0, x1, y1\n",
    "    \n",
    "    def get_attr(self, attr_str):\n",
    "        mask = np.zeros(341)\n",
    "        #print(attr_str)\n",
    "        if attr_str == '' or (isinstance(attr_str, float) and  math.isnan(attr_str)):\n",
    "          return mask\n",
    "        else:\n",
    "          attr_list = [int(x) for x in attr_str.split(\",\")]\n",
    "          mask[attr_list] = 1\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        image_entry = self.imgs[idx]\n",
    "        #masks = mask == obj_ids[:, None, None]\n",
    "        img = Image.open(os.path.join(self.image_dir, image_entry['file_name'] + \".jpg\")).convert(\"RGB\")\n",
    "        #img = img.resize()\n",
    "        #boxes = torch.as_tensor(image_entry['boxes'], dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.as_tensor(image_entry['labels'], dtype=torch.int64)\n",
    "        #print(image_entry['labels'])\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        attributes = []\n",
    "        for entry in image_entry['masks']:\n",
    "            masks.append(self.rle_decode_string(entry, image_entry['height'], image_entry['width']))\n",
    "            boxes.append(list(self.rle2bbox(entry, (image_entry['height'], image_entry['width']))))\n",
    "        #print(image_entry[\"image_id\"])\n",
    "        for attr in image_entry['attributes']:\n",
    "            attributes.append(self.get_attr(attr))\n",
    "        #print(\"Anmol1: \",idx)\n",
    "        attributes = torch.as_tensor(attributes, dtype=torch.int64)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        #print(\"Anmol2\")\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        #print(masks.shape)\n",
    "        #print(\"Image: \", np.array(img).shape)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(image_entry['labels'])), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"attributes\"] = attributes\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            #print(img.size)\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17615,
     "status": "ok",
     "timestamp": 1590100178639,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "ntFhycIWvsp-",
    "outputId": "280798df-59d8-4630-efe8-b052dfab2674"
   },
   "outputs": [],
   "source": [
    "test1 = FashionDataset(fashion_dict, os.path.join(dataDir, 'mini_kaggle', 'train'))\n",
    "print(test1[0][1][\"attributes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVvAiqWbvalN"
   },
   "outputs": [],
   "source": [
    "#from models.detection.faster_rcnn import FastRCNNPredictor\n",
    "#from models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(pretrained_backbone=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384,
     "status": "ok",
     "timestamp": 1590102526484,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "IOMArj8axg1A",
    "outputId": "587e796f-d371-4d50-9129-7419a813cb30"
   },
   "outputs": [],
   "source": [
    "model = get_instance_segmentation_model(47)\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZs5BiwevalS"
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0, 'vision/references/detection')\n",
    "\n",
    "#import transforms as T\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, min_size, max_size):\n",
    "        if not isinstance(min_size, (list, tuple)):\n",
    "            min_size = (min_size,)\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        #print(image.size)\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        #print(im_shape)\n",
    "        min_size = float(torch.min(im_shape))\n",
    "        max_size = float(torch.max(im_shape))\n",
    "        #print(\"Min:\", min_size, \"Max:\", max_size)\n",
    "        scale_factor = self.min_size[0] / min_size\n",
    "        #print(\"Scale factor: \", scale_factor)\n",
    "        if max_size * scale_factor > self.max_size:\n",
    "            scale_factor = self.max_size / max_size\n",
    "        image = torch.nn.functional.interpolate(image[None], scale_factor=scale_factor, mode='bilinear',align_corners=False)[0]\n",
    "        new_shape = torch.tensor(image.shape[-2:])\n",
    "        #print(\"New Shape: \", new_shape)\n",
    "        if target is None:\n",
    "            return image, target\n",
    "\n",
    "        if \"masks\" in target:\n",
    "            mask = target[\"masks\"]\n",
    "            mask = misc_nn_ops.interpolate(mask[None].float(), scale_factor=scale_factor)[0].byte()\n",
    "            target[\"masks\"] = mask\n",
    "        if \"boxes\" in target:\n",
    "            ratios = [ torch.tensor(s, dtype=torch.float32) / torch.tensor(s_orig, dtype=torch.float32) \n",
    "                      for s, s_orig in zip(new_shape, im_shape)]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            #ratio_height, ratio_width = float(new_shape[0])/float(im_shape[0]), new_shape[1]/im_shape[1]\n",
    "            #print(\"Ratio Height: \", ratio_height, \"Ratio Width: \", ratio_width)\n",
    "            xmin, ymin, xmax, ymax = target[\"boxes\"].unbind(1)\n",
    "            #print(\"Xmin Before\", xmin, xmin.dtype)\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            #print(\"Xmin After: \", xmin)\n",
    "            target[\"boxes\"] = torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEDPoxAvHz8p"
   },
   "outputs": [],
   "source": [
    "#from engine import train_one_epoch,evaluate\n",
    "#import utils\n",
    "#import transforms as T\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    #transforms.append(T.Resize((400,), 600))\n",
    "    transforms.append(T.ToTensor())\n",
    "    transforms.append(Resize((400,), 600))\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tItAUAt4valV"
   },
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = FashionDataset(fashion_dict[:950], os.path.join(dataDir, 'mini_kaggle', 'train'),get_transform(train=True))\n",
    "dataset_test = FashionDataset(fashion_dict[950:960], os.path.join(dataDir, 'mini_kaggle', 'train'), get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "indices_test = torch.randperm(len(dataset_test)).tolist()\n",
    "#print(len(indices))\n",
    "#print(indices[101:102])\n",
    "dataset = torch.utils.data.Subset(dataset, indices)\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices_test)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=4, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4882,
     "status": "ok",
     "timestamp": 1590102578321,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "PcT4skq9valZ",
    "outputId": "33b89345-edd3-4cf2-979b-b0cc488db263"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 47\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OnqpO6JvtXB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "#import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "#from coco_utils import get_coco_api_from_dataset\n",
    "#from coco_eval import CocoEvaluator\n",
    "#import utils\n",
    "@torch.no_grad()\n",
    "def evaluate2(model, data_loader, device):\n",
    "    print(\"Testing - ANMOL\")\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    #ANMOLcoco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    #ANMOLiou_types = _get_iou_types(model)\n",
    "    #ANMOLcoco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "    res_list= []\n",
    "    idx = 0\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        #print(res)\n",
    "        res_list.append(res)\n",
    "        evaluator_time = time.time()\n",
    "        #ANMOLcoco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    print(res_list)\n",
    "    #ANMOLcoco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    #ANMOLcoco_evaluator.accumulate()\n",
    "    #ANMOLcoco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return\n",
    "    #ANMOLreturn coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1012633,
     "status": "error",
     "timestamp": 1590103610941,
     "user": {
      "displayName": "Anmol Gupta",
      "photoUrl": "",
      "userId": "15196836321176155207"
     },
     "user_tz": 420
    },
    "id": "K_S-IvD5LmpE",
    "outputId": "1222b388-94ac-4625-bd9a-9f2e2907116e"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "import math\n",
    "for epoch in range(num_epochs):\n",
    "  # train for one epoch, printing every 10 iterations\n",
    "  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "  # update the learning rate\n",
    "  lr_scheduler.step()\n",
    "  evaluate2(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vPTXXG6valg"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"output/pytorch_method.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sm57Qknrvalk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "attr_class_pytorch_method_ngc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
