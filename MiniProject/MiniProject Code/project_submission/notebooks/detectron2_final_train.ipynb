{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL Uncomment the first time to install\n",
    "#!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
    "# UNINSTALL PYCOCOTOOLS IF YOU ALREADY HAVE IT INSTALLED\n",
    "#!pip uninstall pycocotools --yes\n",
    "import sys\n",
    "import os\n",
    "cs231_repo_path = \"/mount/ws/code_devel/cs231n-project\"\n",
    "sys.path.append(cs231_repo_path)\n",
    "#detectron_path = os.path.join(cs231_repo_path, \"detectron2_code\")\n",
    "#!cd {detectron_path} && python -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall detectron2 --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import gc\n",
    "import pycocotools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: ADD PATH TO DATASET\n",
    "# DIRECTORY SHOULD HAVE ['train', 'sample_submission.csv', 'train.csv', 'label_descriptions.json', 'test']\n",
    "dataDir = \"/mount/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL:This Cell is needed to get names of object and attribute classes\n",
    "# Dictionaries: idx_to_did and did_to_idx are important for Training Attr Classifier\n",
    "with open(os.path.join(dataDir, 'label_descriptions.json'), 'r') as file:\n",
    "    label_description = json.load(file)\n",
    "#label_description\n",
    "n_classes = len(label_description['categories'])\n",
    "n_attributes = len(label_description['attributes'])\n",
    "print(F\"Classes: {n_classes}, Attributes: {n_attributes}\")\n",
    "categories_df = pd.DataFrame(label_description['categories'])\n",
    "attributes_df = pd.DataFrame(label_description['attributes'])\n",
    "idx_to_did = {}\n",
    "did_to_idx = {}\n",
    "for idx, entry in attributes_df.iterrows():\n",
    "    idx_to_did[idx] = entry['id']\n",
    "    did_to_idx[entry['id']] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: This is processed version of train.csv. This combines all the segments, classes, boxes and attributes into\n",
    "# one single list entry which makes it a bit easier to process. I have uploaded 2 json on cloud (one with 2000 \n",
    "# images and other with 5000 images)\n",
    "wsDir = \"/mount/ws/attr_cls_test\"\n",
    "fashion_dict = []\n",
    "with open(os.path.join(wsDir, 'fashion_training.json'), 'r') as file:\n",
    "    fashion_json = json.load(file)\n",
    "fashion_dict = fashion_json[\"train\"]\n",
    "# Just sanity check\n",
    "print(fashion_dict[0].keys())\n",
    "print(fashion_dict[0]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block has all the functionality to read process the data: Masks, Bounding Boxes, Attributes \n",
    "# that is consumed by dataloader\n",
    "\n",
    "import math, numpy\n",
    "\n",
    "def rle_decode_string(string, h, w):\n",
    "        mask = np.full(h*w, 0, dtype=np.uint8)\n",
    "        annotation = [int(x) for x in string.split(' ')]\n",
    "        for i, start_pixel in enumerate(annotation[::2]):\n",
    "            mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "        mask = mask.reshape((h, w), order='F')\n",
    "        return mask\n",
    "\n",
    "def rle2bbox(rle, shape):\n",
    "    '''\n",
    "        Get a bbox from a mask which is required for Detectron 2 dataset\n",
    "        rle: run-length encoded image mask, as string\n",
    "        shape: (height, width) of image on which RLE was produced\n",
    "        Returns (x0, y0, x1, y1) tuple describing the bounding box of the rle mask\n",
    "        \n",
    "        Note on image vs np.array dimensions:\n",
    "        \n",
    "            np.array implies the `[y, x]` indexing order in terms of image dimensions,\n",
    "            so the variable on `shape[0]` is `y`, and the variable on the `shape[1]` is `x`,\n",
    "            hence the result would be correct (x0,y0,x1,y1) in terms of image dimensions\n",
    "            for RLE-encoded indices of np.array (which are produced by widely used kernels\n",
    "            and are used in most kaggle competitions datasets)\n",
    "    '''\n",
    "        \n",
    "    a = np.fromiter(rle.split(), dtype=np.uint)\n",
    "    a = a.reshape((-1, 2))  # an array of (start, length) pairs\n",
    "    a[:,0] -= 1  # `start` is 1-indexed\n",
    "\n",
    "    y0 = a[:,0] % shape[0]\n",
    "    y1 = y0 + a[:,1]\n",
    "    if np.any(y1 > shape[0]):\n",
    "        # got `y` overrun, meaning that there are a pixels in mask on 0 and shape[0] position\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "    else:\n",
    "        y0 = np.min(y0)\n",
    "        y1 = np.max(y1)\n",
    "\n",
    "    x0 = a[:,0] // shape[0]\n",
    "    x1 = (a[:,0] + a[:,1]) // shape[0]\n",
    "    x0 = np.min(x0)\n",
    "    x1 = np.max(x1)\n",
    "\n",
    "    if x1 > shape[1]:\n",
    "        # just went out of the image dimensions\n",
    "        raise ValueError(\"invalid RLE or image dimensions: x1=%d > shape[1]=%d\" % (\n",
    "            x1, shape[1]\n",
    "        ))\n",
    "\n",
    "    return int(x0), int(y0), int(x1), int(y1)\n",
    "    \n",
    "def get_attr(attr_str, max_class, labels_length):\n",
    "    if attr_str == '' or (isinstance(attr_str, float) and  math.isnan(attr_str)):\n",
    "        return [max_class for i in range(labels_length)]\n",
    "    else:\n",
    "        attr_list = [did_to_idx[int(x)] for x in attr_str.split(\",\")]\n",
    "        fillers = [max_class for i in range(labels_length - len(attr_list))]\n",
    "        attr_list.extend(fillers)\n",
    "        return attr_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: MAKE SURE YOU dataDir variable is set above dataDir = \"/mount/data/train\"\n",
    "\n",
    "# https://detectron2.readthedocs.io/tutorials/datasets.html\n",
    "# https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
    "import os\n",
    "from detectron2.structures import BoxMode\n",
    "import pycocotools\n",
    "train_path = \"/mount/data/train\"\n",
    "\n",
    "def get_fashion_dict(df):\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    path_name = train_path\n",
    "    \n",
    "    for idx, df_entry in enumerate(df):\n",
    "        \n",
    "        record = {}\n",
    "        \n",
    "        # Convert to int otherwise evaluation will throw an error\n",
    "        record['height'] = int(df_entry['height'])\n",
    "        record['width'] = int(df_entry['width'])\n",
    "        \n",
    "        record['file_name'] = os.path.join(path_name, df_entry['file_name']+\".jpg\")\n",
    "        record['image_id'] = idx\n",
    "        \n",
    "        objs = []\n",
    "        for mask_entry, class_entry, attr_entry in zip(df_entry['masks'], df_entry['labels'], df_entry['attributes']):\n",
    "            #print(mask_entry)\n",
    "            mask = rle_decode_string(mask_entry, record['height'], record['width'])\n",
    "            # opencv 4.2+\n",
    "            # Transform the mask from binary to polygon format\n",
    "            contours, hierarchy = cv2.findContours((mask).astype(np.uint8), cv2.RETR_TREE,\n",
    "                                                    cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # opencv 3.2\n",
    "            # mask_new, contours, hierarchy = cv2.findContours((mask).astype(np.uint8), cv2.RETR_TREE,\n",
    "            #                                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "            segmentation = []\n",
    "\n",
    "            for contour in contours:\n",
    "                contour = contour.flatten().tolist()\n",
    "                # segmentation.append(contour)\n",
    "                if len(contour) > 4:\n",
    "                    segmentation.append(contour)  \n",
    "            obj = {\n",
    "                'bbox': list(rle2bbox(mask_entry, (record['height'], record['width']))),\n",
    "                'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                'category_id': class_entry,\n",
    "                'attributes': get_attr(attr_entry, 294, 14),\n",
    "                'segmentation': segmentation,\n",
    "                'iscrowd': 0\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record['annotations'] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: THIS IS JUST FOR Visualization later after the network is trained\n",
    "d2_ds = get_fashion_dict(fashion_dict[0:10])\n",
    "print(len(d2_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: This Structure is used to store Attributes. getitem, get_tensor are important\n",
    "\n",
    "from typing import Iterator, List, Tuple, Union\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Base Attribute holder\n",
    "class Attributes:\n",
    "    \"\"\"\n",
    "    This structure stores a list of attributes as a Nx14 torch.\n",
    "    Int training dataset, there were a maximum of 14 attributes per instance\n",
    "    https://detectron2.readthedocs.io/_modules/detectron2/structures/boxes.html#Boxes\n",
    "    It behaves like a Tensor\n",
    "    (support indexing, `to(device)`, `.device`, `non empty`, and iteration over all attributes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tensor: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor[float]): a Nx14 matrix.  Each row is [attribute_1, attribute_2, ...].\n",
    "        \"\"\"\n",
    "        device = tensor.device if isinstance(tensor, torch.Tensor) else torch.device(\"cpu\")\n",
    "        tensor = torch.as_tensor(tensor, dtype=torch.int64, device=device)\n",
    "        assert tensor.dim() == 2, tensor.size()\n",
    "\n",
    "        self.tensor = tensor\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> \"Attributes\":\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Attributes: Create a new :class:`Attributes` by indexing.\n",
    "        The following usage are allowed:\n",
    "        1. `new_attributes = attributes[3]`: return a `Attributes` which contains only one Attribute.\n",
    "        2. `new_attributes = attributes[2:10]`: return a slice of attributes.\n",
    "        3. `new_attributes = attributes[vector]`, where vector is a torch.BoolTensor\n",
    "           with `length = len(attributes)`. Nonzero elements in the vector will be selected.\n",
    "        Note that the returned Attributes might share storage with this Attributes,\n",
    "        subject to Pytorch's indexing semantics.\n",
    "        \"\"\"\n",
    "        \n",
    "        b = self.tensor[item]\n",
    "        assert b.dim() == 2, \"Indexing on Attributes with {} failed to return a matrix!\".format(item)\n",
    "        return Attributes(b)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tensor.shape[0]\n",
    "    \n",
    "    def to(self, device: str) -> \"Attributes\":\n",
    "        return Attributes(self.tensor.to(device))\n",
    "\n",
    "\n",
    "    def nonempty(self, threshold: float = 0.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Find attributes that are non-empty.\n",
    "        An attribute is considered empty if its first attribute in the list is 294.\n",
    "        Returns:\n",
    "            Tensor:\n",
    "                a binary vector which represents whether each attribute is empty\n",
    "                (False) or non-empty (True).\n",
    "        \"\"\"\n",
    "        attributes = self.tensor\n",
    "        first_attr = attributes[:, 0]\n",
    "        keep = (first_attr != 294)\n",
    "        return keep\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Attributes(\" + str(self.tensor) + \")\"\n",
    "\n",
    "    def get_shape(self) -> Tuple[int, int]:\n",
    "        return self.tensor.shape[0], self.tensor.shape[1]\n",
    "    \n",
    "    def get_tensor(self) -> torch.Tensor:\n",
    "        return self.tensor\n",
    "    \n",
    "    def remove_padding(self, attribute):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.tensor.device\n",
    "\n",
    "    def __iter__(self) -> Iterator[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Yield attributes as a Tensor of shape (14,) at a time.\n",
    "        \"\"\"\n",
    "        yield from self.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: This is used to add gt_attributes to dataset mapper.\n",
    "\n",
    "import copy\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "\n",
    "\n",
    "class DatasetMapper:\n",
    "    \"\"\"\n",
    "    A callable which takes a dataset dict in Detectron2 Dataset format,\n",
    "    and map it into a format used by the model.\n",
    "    Derived From: https://github.com/facebookresearch/detectron2/blob/master/detectron2/data/dataset_mapper.py\n",
    "    Just Add Attributes field to instances  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, is_train=True):\n",
    "\n",
    "        self.crop_gen = None\n",
    "\n",
    "        self.tfm_gens = utils.build_transform_gen(cfg, is_train)\n",
    "\n",
    "        # fmt: off\n",
    "        self.img_format     = cfg.INPUT.FORMAT\n",
    "        self.mask_on        = cfg.MODEL.MASK_ON\n",
    "        self.mask_format    = cfg.INPUT.MASK_FORMAT\n",
    "        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON\n",
    "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
    "        # fmt: on\n",
    "        self.keypoint_hflip_indices = None\n",
    "\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n",
    "\n",
    "        Returns:\n",
    "            dict: a format that builtin models in detectron2 accept\n",
    "        \"\"\"\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        # USER: Write your own image loading if it's not from a file\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
    "        utils.check_image_size(dataset_dict, image)\n",
    "\n",
    "        if \"annotations\" not in dataset_dict:\n",
    "            image, transforms = T.apply_transform_gens(\n",
    "                ([self.crop_gen] if self.crop_gen else []) + self.tfm_gens, image\n",
    "            )\n",
    "        else:\n",
    "            # Crop around an instance if there are instances in the image.\n",
    "            # USER: Remove if you don't use cropping\n",
    "            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n",
    "\n",
    "        image_shape = image.shape[:2]  # h, w\n",
    "\n",
    "        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n",
    "        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n",
    "        # Therefore it's important to use torch.Tensor.\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "\n",
    "\n",
    "        if not self.is_train:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            dataset_dict.pop(\"annotations\", None)\n",
    "            dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "            return dataset_dict\n",
    "\n",
    "        if \"annotations\" in dataset_dict:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            for anno in dataset_dict[\"annotations\"]:\n",
    "                if not self.mask_on:\n",
    "                    anno.pop(\"segmentation\", None)\n",
    "                if not self.keypoint_on:\n",
    "                    anno.pop(\"keypoints\", None)\n",
    "\n",
    "            # USER: Implement additional transformations if you have other types of data\n",
    "            annos = [\n",
    "                utils.transform_instance_annotations(\n",
    "                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n",
    "                )\n",
    "                for obj in dataset_dict.pop(\"annotations\")\n",
    "                if obj.get(\"iscrowd\", 0) == 0\n",
    "            ]\n",
    "            instances = utils.annotations_to_instances(\n",
    "                annos, image_shape, mask_format=self.mask_format\n",
    "            )\n",
    "            # Create a tight bounding box from masks, useful when image is cropped\n",
    "            if self.crop_gen and instances.has(\"gt_masks\"):\n",
    "                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()           \n",
    "\n",
    "            if len(annos) and 'attributes' in annos[0]:\n",
    "    \n",
    "                # get a list of list of attributes\n",
    "                gt_attributes = [x['attributes'] for x in annos]\n",
    "                instances.gt_attributes = Attributes(gt_attributes)\n",
    "                \n",
    "            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\n",
    "        return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: For our on Mapper, we need to define train and test loaders\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "class TrendTrainer(DefaultTrainer):\n",
    "    'A customized version of DefaultTrainer. We add the mapping `DatasetMapper` to the dataloader.'\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=DatasetMapper(cfg))\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(cfg, dataset_name, mapper=DatasetMapper(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of the training data to run experiments\n",
    "# You may set this number based on your dataset size\n",
    "\n",
    "df_copy_train = fashion_dict[:40000].copy()\n",
    "df_copy_test = fashion_dict[40000:42000].copy()\n",
    "print(len(df_copy_train))\n",
    "print(len(df_copy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_copy_test = fashion_dict[40000:40500].copy()\n",
    "#print(len(df_copy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Register the train set metadata\n",
    "for d in ['train']:\n",
    "    DatasetCatalog.register('2sample_fashion_' + d, lambda d=df_copy_train: get_fashion_dict(d))\n",
    "    MetadataCatalog.get(\"2sample_fashion_\" + d).set(thing_classes=list(categories_df.name))\n",
    "fashion_metadata = MetadataCatalog.get(\"2sample_fashion_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the test and set metadata\n",
    "for d in ['test']:\n",
    "    DatasetCatalog.register('2sample_fashion_' + d, lambda d=df_copy_test: get_fashion_dict(d))\n",
    "    MetadataCatalog.get(\"2sample_fashion_\" + d).set(thing_classes=list(categories_df.name))\n",
    "fashion_metadata = MetadataCatalog.get(\"2sample_fashion_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = DefaultTrainer(cfg) \n",
    "#from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "#add_trendrcnn_config(cfg)\n",
    "#cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "#cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml\"))\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"2sample_fashion_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "#cfg.DATALOADER.NUM_WORKERS = 1\n",
    "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_3x.yaml\")\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "#cfg.MODEL.WEIGHTS = os.path.join(\"/mount/ws/code_devel/6_2_2020/experiments/rn50_fpn_m1_a0_i1_im5k_lr00025_bs4_mit5k\", \"model_final.pth\")\n",
    "#cfg.MODEL.WEIGHTS = os.path.join(\"/mount/ws/code_devel/6_2_2020/experiments/x101_m1_a0_i1_im5k_lr00025_bs4_mit5k_noAll0_focal_loss/output\", \"model_final.pth\")\n",
    "# ATTRIBUTE CLASSIFIER RELATED CONFIGS: \n",
    "cfg.MODEL.ROI_HEADS.NAME = \"TrendRCNNROIHeads\" # This is needed to running FPN, DC5, X_101 type networks\n",
    "#cfg.MODEL.ROI_HEADS.NAME = \"TrendRes5ROIHeads\" # This is needed to run C4 kind of Network\n",
    "# Number of attribute Classes DONT CHANGE\n",
    "cfg.MODEL.ROI_HEADS.NUM_ATTR_CLASSES = 295\n",
    "# Number of Max Attribute Predictions: DONT CHANGE\n",
    "cfg.MODEL.ROI_HEADS.MAX_ATTR_PRED = 14\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 46  # 46 classes in iMaterialist\n",
    "\n",
    "# These Options modify the Attr Classifier\n",
    "# ATTR_CLS_MODE=0: 1 Linear Layer\n",
    "# ATTR_CLS_MODE=1: Linear->RELU->Linear\n",
    "cfg.MODEL.ROI_HEADS.ATTR_CLS_MODE=1\n",
    "# Decides if Attr Classifier is agnostic to Object Class\n",
    "# If 1, then final Linear Layer Output = 295\n",
    "# if 0, the final Linear Layer Output = object classes (46) * attr_classes (295)\n",
    "cfg.MODEL.ROI_HEADS.ATTR_CLS_AGNOSTIC=0\n",
    "# Decide if we want to ignore object with no Attribute Label in Loss or not\n",
    "# if 0: It will take No Attributes as one of the attr class, account for its loss\n",
    "cfg.MODEL.ROI_HEADS.IGN_NAN_ATTR_CLS =1\n",
    "cfg.MODEL.ROI_HEADS.ATTR_SCORE_THRESH_TEST=0.4\n",
    "\n",
    "##### Input #####\n",
    "# Set a smaller image size than default to avoid memory problems\n",
    "\n",
    "# Size of the smallest side of the image during training\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (800,)\n",
    "# Maximum size of the side of the image during training\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1333\n",
    "# Size of the smallest side of the image during testing. Set to zero to disable resize in testing.\n",
    "cfg.INPUT.MIN_SIZE_TEST = 800\n",
    "# Maximum size of the side of the image during testing\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1333\n",
    "\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
    "#cfg.SOLVER.CHECKPOINT_PERIOD = 25\n",
    "cfg.SOLVER.MAX_ITER = 20000   \n",
    "#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this set you output directory name clearly to store checkpoints\n",
    "#\"rn50_fpn_m1_a0_i1_im5k_lr00025_bs4_mit5k_attrLossSum\"\n",
    "#\"remove_all0attrloss_rn50_fpn_m1_a0_i1_im5k_lr00025_bs4_mit5k\"\n",
    "#cfg.OUTPUT_DIR=\"rn50_fpn_m1_a0_i1_im5k_lr00025_bs4_mit5k_focal_loss_retinanet\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrendTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.resume_or_load(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ON TRAINED WEIGHTS\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set the testing threshold for this model\n",
    "\n",
    "cfg.DATASETS.TEST = ('2sample_fashion_test',)\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Phase\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "#trainer = DefaultTrainer(cfg)\n",
    "#trainer2 = DefaultTrainer(cfg)\n",
    "#trainer2.resume_or_load(resume=False)\n",
    "#trainer = DefaultTrainer(cfg)\n",
    "from detectron2.modeling import build_model\n",
    "model = build_model(cfg)\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "file_path=os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')\n",
    "DetectionCheckpointer(model).load(file_path)\n",
    "#print(model)\n",
    "evaluator = COCOEvaluator(\"2sample_fashion_test\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"2sample_fashion_test\")\n",
    "inference_on_dataset(model, val_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING THE OUTPUT IMAGE WITH BOXES AND MASKS\n",
    "\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "plt.figure(figsize=(20,20))\n",
    "for d in d2_ds[0:1]:\n",
    "    #random.sample(d2_ds, 1):/\n",
    "    #print(d[\"file_name\"])\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    #print(outputs)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=fashion_metadata, \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANMOL: I use this to manually analyze and compare ground truth and predicted attributes.\n",
    "# Print outputs variable from the above cell.. \n",
    "\n",
    "for entry in d2_ds[0]['annotations']:\n",
    "    #print(entry['category_id'])\n",
    "    print(F\"CLASS LABEL: {entry['category_id']}, NAME: {categories_df.loc[entry['category_id'], 'name']}\")\n",
    "    #print(categories_df.loc[31, 'name'])\n",
    "    print(entry['attributes'])\n",
    "    for attr in entry['attributes']:\n",
    "        if attr != 294:\n",
    "            print(\"-------\", attributes_df.loc[attr, 'name'])\n",
    "            \n",
    "print(\"---------OUTPUT----------\")\n",
    "output_attr = [[204, 160, 294, 157, 159, 207, 158, 206, 205, 209, 213, 216, 214, 210],\n",
    "        [294, 218, 223, 224, 219, 222, 225, 121, 221,  89, 220, 247, 259,  63],\n",
    "        [294, 163, 162, 166, 164, 170, 173, 168,  47, 161,  36, 167, 200, 210],\n",
    "        [294, 218, 223, 224, 222, 219, 220, 190,  62, 276, 175, 217,  41, 108],\n",
    "        [204, 160, 157, 294, 159, 205, 207, 209, 206, 213, 158, 216, 214, 211],\n",
    "        [294, 174, 175, 176, 177, 178, 162,  80,  22, 151, 138, 284, 263,  52],\n",
    "        [248, 270, 269, 154, 230, 115, 136, 142,  36, 135, 143, 251, 128, 234]]\n",
    "#output_attr = [[294,  56, 160, 277, 268, 227,  64, 168,  77,  95, 256,  12, 273, 156],\n",
    "#               [160, 274, 294,  64, 168, 204, 283,  77, 287, 220, 149, 153, 108,  95],\n",
    "#               [269, 195, 265,   1, 118, 165, 145,  38,  60, 220,  37, 188, 264, 115],\n",
    "#               [294,  90, 274, 166, 268, 287,  89, 160, 283,  41, 206,  64, 143,  84]]\n",
    "#output_class = [31, 31,  1, 31]\n",
    "output_class = [31, 32, 28, 32, 31, 29,  6]\n",
    "for class_id, entry in enumerate(output_attr):\n",
    "    print(\"Class NAME: \", categories_df.loc[output_class[class_id], 'name'])\n",
    "    for idx in entry:\n",
    "        if idx == 294:\n",
    "            continue\n",
    "        print(\"----:\", attributes_df.loc[idx, 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
